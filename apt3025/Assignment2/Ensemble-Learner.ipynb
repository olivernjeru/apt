{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Ensemble Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb_clf = GradientBoostingClassifier()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use scikit-learn to create a synthetic dataset for either classification or regression with 20,000 instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples = 20000, n_features=20, random_state=91)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=91)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an ensemble learner on the above dataset  using three different machine learning algortihms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('gb', GradientBoostingClassifier()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('gb', gdb_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "#Train Ensemble learner on the training data\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the ensemble learner on the training data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier 0.974\n",
      "RandomForestClassifier 0.9756\n",
      "SVC 0.9726\n",
      "VotingClassifier 0.9754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (gdb_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations made\n",
    "1. When i set the random state to be zero when i am declaring x and y, the accuracy scores for the models are average at 0.89s. \n",
    "2. When i set it to be 42, i get 0.91s.\n",
    "3. The sweet spot with the highest accuracy scores for me was a randomstate=91 where the votingClassifier had a score of 0.975.\n",
    "4. I used three different algos, Gradient Boosting Classifier that combines weaker learners into a strong learner with each new one being trying to correct its predecessor. I also used the Random Forest Classifier which is an ensemble (group) of decision trees that use bagging and or pasting. I also used a Support Vector Machine Classifier which is known to be effective in high dimensional spaces. Please note that all these algos are classification algorithms because my dataset is a classification one, make_classification.\n",
    "5. I used hard voting that essentially sums up the predictions of each classifier and then predicts the class that gets the most votes. In this case, Random Forest Classifier got the most votes from the test accuracy scores with a 97.5% accuracy score.\n",
    "6. My conclusion is that my model is okay, I would personally prefer a prediction of 99.5 and above but I guess I can work with this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the Jupyter notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
