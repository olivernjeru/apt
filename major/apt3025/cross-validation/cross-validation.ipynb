{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic Machine Learning Process\n",
    "To evaluate our supervised models, we have split our dataset into a training set and a test set using the train_test_split function.\n",
    "\n",
    "We then build a model on the training set by calling the fit method, and evaluate it on the test set using the score method, which for classification computes the fraction of correctly classified samples.\n",
    "\n",
    "Below is an example of that process: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# create a synthetic dataset\n",
    "X, y = make_blobs(random_state = 0)\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=44)\n",
    "# instantiate a model and fit it to the training set\n",
    "logreg = LogisticRegression().fit(X_train, y_train)\n",
    "# evaluate the model on the test set\n",
    "print(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we split the data\n",
    "We are interested in measuring how well our model generalizes to new, previously unseen data.\n",
    "\n",
    "We are not interested in how well our model fits the training set, but rather in how well it can make predictions for data that was not observed during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Evaluation Approaches\n",
    "\n",
    "### Cross-Validation(A More Robust Way to Assess Generalization Performance)\n",
    "\n",
    "### Grid Search(An Effective method for adjusting parameters in supervised models for the best generalization performance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "logreg = LogisticRegression(max_iter=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n",
      "1.0\n",
      "0.9333333333333333\n",
      "0.9666666666666667\n",
      "1.0\n",
      "Average cross-validation score: 0.97\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=5)\n",
    "for score in scores:\n",
    "    print(score)\n",
    "print(\"Average cross-validation score: {:.2f}\".format(scores.mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
